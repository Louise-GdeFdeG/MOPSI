# MOPSI - 12/11/2019 Notes

## Elaboration of a strategy for the tests
We will test our network on several functions. It is **essential** to save our data (in case we want to work again on those data later on).  
For each function, we test on the same parameters and change one at the time. In order to have clear methodology, we will work following this instructions :
* W = [5, 10, 50, 100]
* L = [2, 5, 7, 10]
* N = [100, 500, 1000, 2000]
* NB_EPOCH = [1, 5, 10, 50, 100]
* Learning_rate = [1e-4, 1e-3, 1e-2]

Additionally, in order to have a clear organization, we will gather our pre_processing files in one repository. Testing and training must be done in separate files but must be general (we must be able to use them regardless of the function).

Our first objectives are :
1. A constant function
2. A function f defined by f(x) = A_2°ReLU°A_1°ReLU(x) (the same form as the network we are working with)
3. A infinite-continuous function (we'll take a polynomial)
4. A piecewise continuous function.

## Next steps
* Read the documentation about the Adam optimizer, how does it work? Difference with the stochastic gradient ?
* How initialize the coefficients of the layers? For now, they are randomly initialized (this is why sometimes we get really different results for the same function when we run the training process several times). **=> Different initialization should yield to the same result**. We will thus have to verify that for different initial values we found the same minimizer ( = the layers coefficients of the network) at the end.